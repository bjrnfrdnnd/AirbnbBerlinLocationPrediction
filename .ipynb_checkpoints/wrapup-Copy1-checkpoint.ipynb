{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Prediction for Airbnb Berlin Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to predict whether or not a given combination of features (price, bedroom, room_type, accommodates) and locations (latitude, longitude) exists in a given Airbnb Listing.\n",
    "More precisely, we are going to estimate the probability that such a combination exists in the listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date of request has to be similar to some dates in the listings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that Rob tells us the features of his whereabouts at a date that is close to given dates in the listings. This means we are not going to exprapolate the evolution of prices. We are therefore only learning from existing places and locations at dates close to that at which Rob gives us the information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Room type cannot be different from those in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are training on available data only and do not make assumptions concerning extrapolation, we can only predict on categorical data that is in the range of the training data. \n",
    "\n",
    "Note that this assumption is kind of valid also for non-categorical data such as price, number of bedrooms or accommodates, and geographical position. However, we assume a somewhat smooth dependence on these non-categorical variables that will allow us to predict on values of price, bedrooms, accommodates, latitude and longitude that were not available in the original data. Note that combinations of features/locations that were not existing in the original dataset are considered to be negative samples. We use these samples to construct rows in the dataset that builds on existing values, yet its label is 'not in the listings'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a random forest on the available data to make the predictions. Core results:\n",
    "* Predictions are better than those generated by random guesses\n",
    "* Forests trained on balanced data can make useful predictions on unbalanced data\n",
    "* The final probability finally attributed to a given combination of features and locations **depends on the multiplicity of negative samples in the dataset to be evaluated**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is treated as a binary classification problem. We consider each sample that exists in the listings as positive sample. Each combination of features and locations that is not in the listings is considered a negative sample. We construct negative samples by combining existing features and existing locations such that these combinations are not present in the listings. We then use datasets with a freely choosable ratio between negative and positive samples (in certain limits given by the total number of possible combinations between distinct features and distinct locations) to train random forests. The resulting forest will be able to guess the probaility of an unseen(meening not present in the listings) combination of features/locations (which might include unseen features and/or unseen locations) to be present in the listings.\n",
    "\n",
    "If the features/location combination that is requested exists in the listings (given a range of dates), we will simply perform a lookup on the existing data in the listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictive power of random forests was far better than that of neural networks or gradient boosting (results not shown). In particular, they are far faster to train for this simple classification task.\n",
    "\n",
    "The forest does not make directly a prediction whether or not a set of features/locations is in the listings. It rather yields a number between 0 and 1 which can be interpreted as a probability (the score requested in the homework). To be able to predict a class given that probability (in the listings [1] or not [0]), one has to choose a **threshold** on the probability. In the evaluation section, We compare some metrics such as precision, recall, and f1 in dependence of the choice of threshold.\n",
    "\n",
    "We find that the predictive power of the forest (measured in terms of the metrics at threshold 0.5) is mostly larger than the metrics evaluated with a model that randomly chooses whether or not a set of features/locations is in the listings or not. Here. the probability $p$ of a given combination being in the listings is considered to be the ratio of positive samples in the dataset ($N_+$) divided by the total number of samples in the dataset ($N_t$): $p=N_+/N_t$.\n",
    "\n",
    "Under this assumption, we can calculate the expected value of the metrics $precision$, $recall$, and $f_1$. The predictive power of the random forest ($f_{precision}$) is then given as $$f_{precision} = precision_{forest}/precision_{random}\\quad.$$\n",
    "\n",
    "We also calculate the corresponding quantities for $recall$ and $f_1$.\n",
    "\n",
    "We find that the metrics for a given balancedness of valuation data strongly depends on both the threshold **and the balancedness of the training data**. While the best performance is achieved when the same balancedness is used for both training and valuation data, we find that we can get close to those values using balanced training and unbalanced valuation data for a threshold of 0.5, if **we perform an analytical transformation of the probabilities of the forest that depends on how strongly we undersampled the possible negative samples in order to achieve a balanced training dataset __[link to publication](https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf)__**. \n",
    "\n",
    "We can understand the dependence of the predicted probability based on the ratio in the **validation** data in the following way: Imagine that you are to check whether or not a list of combinations of one feature and one million locations matches any of the listings in the Airbnb database. Imagine that all the locations are closely clustered around just one house in Berlin. Imagine that only one of these combinations actually corresponds to a listing in the database. It is then not conceivable that the prediction machine trained on a dataset with a vastly different ratio would make adequate predictions as it is not used to make location discrimations at this scale.\n",
    "\n",
    "As a consequence, this means that we **cannot give** useful probabilities unless **we know beforehand the ratio of positive to negative samples in the evaluation dataset**. In other words, we would assign a **different** probability to the same combination of features/locations if the ratio of positive samples to negative samples in the dataset to be evaluated changes.\n",
    "\n",
    "Finally, the predictive power also depends on the granularity of location discretization. While it is technically absolutely possible to use training data with one degree of granularity, and predict on valuation data of a different granularity, our tests have shown that the loss of power of prediction (as compared to the best achievable power at the same granularity and balancedness) is quite large (not shown).\n",
    "\n",
    "We conclude:\n",
    "\n",
    "**using the analytic transform, the probabilities generated by a forest trained with balanced data can be adjusted to yield reasonable values on unbalanced evaluation data; however, it is necessary to know how unbalanced the evaluation data is in order to make the adjustment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing some needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LocationPrediction.source_data\n",
    "import LocationPrediction.preprocess_data\n",
    "import LocationPrediction.predictor\n",
    "from LocationPrediction.GmapsAirbnbExplorer import AirbnbExplorer\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "import numpy as np\n",
    "from  IPython.display import display\n",
    "import gmaps\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data downloading and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now downloading the data using an instance of the custom-made class `source_dataClass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = LocationPrediction.source_data.source_dataClass()\n",
    "# download and clean data\n",
    "# df_clean_csv = sd.download_and_clean() # takes time because it reads individual source csv files, combines them, cleans them\n",
    "# read the cleaned csv into memory\n",
    "df_clean_csv = sd.read_clean_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this procedure, we have produced a clean csv file consiting of the concatenation of all source zip files. It is saved to disk and also exists in memory as a pandas DataFrame (`df_clean_csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 292651 entries, 0 to 292650\n",
      "Data columns (total 11 columns):\n",
      "room_type         292651 non-null object\n",
      "accommodates      292651 non-null float64\n",
      "bedrooms          292651 non-null float64\n",
      "price             292651 non-null float64\n",
      "latitude          292651 non-null float64\n",
      "longitude         292651 non-null float64\n",
      "last_modified     292651 non-null datetime64[ns]\n",
      "survey_id         292651 non-null int64\n",
      "year              292651 non-null int64\n",
      "yearmonth         292651 non-null int64\n",
      "yearmonthIndex    292651 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(5), int64(4), object(1)\n",
      "memory usage: 24.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clean_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most demanding tasks in this section are \n",
    "* sampling of negative samples. How do I choose a number of samples without replacement out of a list that is too large to be kept in memory (number of distinct features multiplied by number of distinct locations), while a low number of discrete values in this list is marked as positive and should not be drawn? See the code for details of our approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we allow to draw samples using an instance of the custom class `preprocess_dataClass`. We have 5 parameters:\n",
    "* `min_date`: only sample from dates larger or equal to this date\n",
    "* `max_date`: only sample from dates smaller or equal to this date\n",
    "* `n_cuts`: the level of granularity of the location data. `n_cuts`=2 means that all available location data will be mapped to a 2x2 grid of Berlin. If this setting is set to -1, no discretization is performed.\n",
    "* `N_positive_samples_to_draw`: the number of positive samples that should exist in the training dataset. If this number is -1, all samples in the given date range are drawn. If the number is larger than the total number of available rows in the given date range, only the available number is being drawn (i.e., all samples are drawn but not more, we require uniqueness).\n",
    "* `Neg_Multiplier`: gives the ratio of Negative Samples to Positive samples in the final training data. If this ratio is 2, there will be twice as many negative samples as there are positive samples in the training data. If it is -1, the ratio of positive to negative samples in the training data will be identical to the same ratio in the base data (all possible combinations of disctinct features and distinct locations). If the number yields a target number of negative samples that is larger than the available negative samples (`N_distinct_features`*`N_distinct_lcoations`-`N_positive`), the value is capped such that not more negative samples as given by this number are being drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_selection_positives: 16732\n",
      "N_selection_distinct_features: 7114\n",
      "N_selection_distinct_locations: 22\n",
      "N_selection_distinct_combinations: 156508\n",
      "N_selection_possible_negatives: 139776\n",
      "N_training positives: 16732\n",
      "N_training negatives: 16732\n",
      "N_training total: 33464\n",
      "N_training distinct features: 7114\n",
      "N_training distinct locations: 22\n",
      "beta_for_balanced_data: 0.11970581501831502\n",
      "beta_for_training_data: 0.11970581501831502\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>room_type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Private room</td>\n",
       "      <td>52.5595</td>\n",
       "      <td>13.2985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Shared room</td>\n",
       "      <td>52.4985</td>\n",
       "      <td>13.4255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Private room</td>\n",
       "      <td>52.3755</td>\n",
       "      <td>13.5525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.5595</td>\n",
       "      <td>13.4255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Shared room</td>\n",
       "      <td>52.4985</td>\n",
       "      <td>13.4255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  price  bedrooms  accommodates        room_type  latitude  longitude\n",
       "0      1   44.0       1.0           3.0     Private room   52.5595    13.2985\n",
       "1      1   56.0       1.0           3.0      Shared room   52.4985    13.4255\n",
       "2      1   24.0       1.0           1.0     Private room   52.3755    13.5525\n",
       "3      1   97.0       0.0           4.0  Entire home/apt   52.5595    13.4255\n",
       "4      1   16.0       1.0           4.0      Shared room   52.4985    13.4255"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data preprocessing up to the generation of a training dataframe\n",
    "prpdata_train = LocationPrediction.preprocess_data.preprocess_dataClass(df=df_clean_csv)\n",
    "\n",
    "# create a reduced dataset by restricting observations to fall into a specific date range\n",
    "# this also creates distinct indices\n",
    "min_date = '2017-02-01'\n",
    "max_date = None\n",
    "n_cuts = 5\n",
    "N_positive_samples_to_draw = -1\n",
    "Neg_Multiplier = 1\n",
    "prpdata_train.create_training_data(min_date=min_date,\n",
    "                                   n_cuts=n_cuts,\n",
    "                                   N_positive_samples_to_draw=N_positive_samples_to_draw,\n",
    "                                   Neg_Multiplier=Neg_Multiplier)\n",
    "\n",
    "print(prpdata_train)\n",
    "display(prpdata_train.df_training_categorical.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this operation are summarized above. The resulting dataset contains 7 columns: the label, the feature columns, and the location columns. Only the room_type column is considered categorical and will be transformed to a one_hot_vector for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training dataset and fit a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue by taking the training data and splitting it into predictive features (`X`: features and locations) and the resulting class (`y`: `label`). Note that we are not creating a separate validation dataset. The validation has been performed beforehand (not shown); some of the results are shown in the evaluation section.\n",
    "\n",
    "We first create a random forest (prior tests have shown that a number of estimators of 500 is reasonable for most situations; for better performance, the number of estimators should be higher than the expected ratio of Negative to Positive samples).\n",
    "\n",
    "The default settings of the RandomForestClassifier have proven to show reasonable results. \n",
    "\n",
    "Finally, we save the resulting forest to disk. Note that this training, performed on large datasets, can take quite a while (in particular if one performs training on unbalanced data which can be quite large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a random forest and train it with the training data\n",
    "do_train = True\n",
    "if do_train:\n",
    "    rf = RandomForestClassifier(n_estimators=500,\n",
    "                                     random_state=42,\n",
    "                                     class_weight='balanced',\n",
    "                                     n_jobs=-1)\n",
    "\n",
    "    df_train = prpdata_train.df_training_one_hot_vector.copy()\n",
    "    X_df_train = df_train.drop(columns=[prpdata_train.label_column_name])\n",
    "    X_train = X_df_train.values\n",
    "\n",
    "    y_df_train = df_train.drop(columns=df_train.columns.difference([prpdata_train.label_column_name]))\n",
    "    y_train = y_df_train.values\n",
    "\n",
    "    rf.fit(X_train,y_train.ravel())\n",
    "\n",
    "    filename = 'my_rf.pkl'\n",
    "    pickle.dump(rf, open(filename, 'wb'))\n",
    "else:\n",
    "    filename = 'my_rf.pkl'\n",
    "    rf = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the random forest for a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to the prediction process. We first create a dataframe that only contains the samples that are considered to be positive (as the prediction method will try to find matching combinations of features/locations in this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction using the trained random forest\n",
    "# the features of the first row of the training data\n",
    "df_with_known_matches = prpdata_train.df_selection_categorical_discretized_locations.copy()\n",
    "df_with_known_matches = df_with_known_matches[prpdata_train.columns_to_keep_categorical].drop_duplicates()\n",
    "df_with_known_matches = df_with_known_matches[df_with_known_matches[prpdata_train.label_column_name]==1].drop(columns=[prpdata_train.label_column_name])\n",
    "# display(df_with_known_matches.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are then instantiating an instance of the custom class `predictorClass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a prediction object\n",
    "predictor = LocationPrediction.predictor.predictorClass(model=rf,\n",
    "                                                        preprocessed_data_for_training=prpdata_train,\n",
    "                                                        df_with_known_matches=df_with_known_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now come to the specification of the valuation data as required by this homework task. Rob gives us a specification of features (saved in my_dict). We allow to specify as many locations as one wishes by passing a dataframe to the method `set_locations_for_predictions`.\n",
    "\n",
    "For simplicity, by default, we take all existing locations that are present in the training dataset (argument `None` to that method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features and locations for which we wish to predict\n",
    "my_dict = {'price': 50.5,\n",
    "           'room_type':'Entire home/apt',\n",
    "           'bedrooms': 1,\n",
    "           'accommodates': 2}\n",
    "predictor.set_features_for_prediction(int_or_dict=my_dict)\n",
    "predictor.set_locations_for_prediction(df_with_locations=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are then creating all possible combinations between the set of features (one row) and the set of locations (arbitrary number of rows). The resulting dataframe contains combinations of features/locations and has the same number of rows as the number of locations that have been passed to the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the cross join of features and locations to generate a dataframe for prediction\n",
    "predictor.set_features_and_locations()\n",
    "# display(predictor.df_features_and_locations_to_predict)\n",
    "# display(predictor.df_features_and_locations_one_hot_vector_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now proceeding to the prediction. Given the trained forest and the dataframe concerning the features given by Rob and the locations that we passed, we calculate a probability that this combination exists in the listings for each row. We adjust these probabilities by using an analytic function that depends on `beta` that adjusts for undersampling (as explained above). This number would have to be adjusted if the ratio of negative to positive samples that are in the evaluation set is different from that of the training set. This is not done here. We only implemented an automatic adjustment of `beta` with respect to the training dataset alone - we assume that the data to be validated has the same ratio of positive to negative samples as the training data. This can be changed easily to reflect the balancedness of the validation set (not done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probabilities to find a given combination of features and locations\n",
    "y_probas =  predictor.predict_probas(beta=prpdata_train.beta_for_training_data)\n",
    "# y_probas = predictor.predict(beta=1)\n",
    "\n",
    "# fill the predictor's dataframe with the results\n",
    "predictor.fill_prediction_dataframe_with_probas(y_probas=y_probas)\n",
    "\n",
    "df_predictions = predictor.df_features_and_locations_to_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this operation is a dataframe containing features/locations, probabilities, and classes (threshold 0.5 using the `beta`-adjusted forest's probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>room_type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>probability</th>\n",
       "      <th>class</th>\n",
       "      <th>prediction_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.4985</td>\n",
       "      <td>13.4255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.4375</td>\n",
       "      <td>13.4255</td>\n",
       "      <td>0.983535</td>\n",
       "      <td>True</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.4985</td>\n",
       "      <td>13.1705</td>\n",
       "      <td>0.359155</td>\n",
       "      <td>False</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.5595</td>\n",
       "      <td>13.4255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.4985</td>\n",
       "      <td>13.2985</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  bedrooms  accommodates        room_type  latitude  longitude  \\\n",
       "0   50.5       1.0           2.0  Entire home/apt   52.4985    13.4255   \n",
       "1   50.5       1.0           2.0  Entire home/apt   52.4375    13.4255   \n",
       "2   50.5       1.0           2.0  Entire home/apt   52.4985    13.1705   \n",
       "3   50.5       1.0           2.0  Entire home/apt   52.5595    13.4255   \n",
       "4   50.5       1.0           2.0  Entire home/apt   52.4985    13.2985   \n",
       "\n",
       "   probability  class prediction_method  \n",
       "0     1.000000   True        prediction  \n",
       "1     0.983535   True        prediction  \n",
       "2     0.359155  False        prediction  \n",
       "3     1.000000   True        prediction  \n",
       "4     1.000000   True        prediction  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print results\n",
    "display(df_predictions.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the dataframe to show the positive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>room_type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>probability</th>\n",
       "      <th>class</th>\n",
       "      <th>prediction_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.4985</td>\n",
       "      <td>13.4255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.4375</td>\n",
       "      <td>13.2985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.5595</td>\n",
       "      <td>13.4255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.4985</td>\n",
       "      <td>13.2985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>52.5595</td>\n",
       "      <td>13.2985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  bedrooms  accommodates        room_type  latitude  longitude  \\\n",
       "0   50.5       1.0           2.0  Entire home/apt   52.4985    13.4255   \n",
       "9   50.5       1.0           2.0  Entire home/apt   52.4375    13.2985   \n",
       "3   50.5       1.0           2.0  Entire home/apt   52.5595    13.4255   \n",
       "4   50.5       1.0           2.0  Entire home/apt   52.4985    13.2985   \n",
       "5   50.5       1.0           2.0  Entire home/apt   52.5595    13.2985   \n",
       "\n",
       "   probability  class prediction_method  \n",
       "0          1.0   True        prediction  \n",
       "9          1.0   True        prediction  \n",
       "3          1.0   True        prediction  \n",
       "4          1.0   True        prediction  \n",
       "5          1.0   True        prediction  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_predictions.sort_values(by='probability',ascending=False).head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical representation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written a custom class that is able to show a heatmap of the results. Predictions and actual results can be graphically compared by slightly changing e.g. the price such that it takes on a value not in the listing (but close to one) and running the above operation twice: once for a feature set in the listings, one slightly outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa950c343db745acbbd213ef1eff1a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Airbnb Berlin Location Prediction</h3><h4>Data from <a href=\"http://tomslee.net…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AirbnbExplorer(df_predictions).render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfdaed75ab9447c9f6cd87e52fd404e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Airbnb Berlin Location Prediction</h3><h4>Data from <a href=\"http://tomslee.net…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AirbnbExplorer(df_predictions).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we show some evaluation of the data. The following are only images, no live evaluation is implemented. The code for this evaluation is based on jupyter notebooks that are organised a little bit differently from the simplified classes implemented here.\n",
    "\n",
    "The following graphs show the dependence of three metrics ($precision$, $recall$, $f_1$) on the threshold. The main idea is to compare the predictive performance of a forest trained on balanced data (green lines) (which is vastly faster than training on complete unbalanced data) to a forest trained on unbalanced data (blue lines).\n",
    "\n",
    "To show the success of the analytical transform mentioned above, we also show the effect of applying this transform (red lines). We see that the application of this transform is very successful, increasing the predictive power of the forest at the threshold 0.5 greatly.\n",
    "\n",
    "We conclude that the transform is a useful tool to take probabilities as given by the forest trained on unbalanced data and transform it to probabilities that serve the evaluation of datasets that are unbalanced.\n",
    "\n",
    "The plots below show both the metric  ($prec$, $reca$, $f_1$) as a function of threshold as well as how much the method is above a random guess ($f\\_prec$, $f\\_reca$, $f\\_1$), where the $f\\_$- values are defined as $metric_{forest}/expected\\_metric\\_when\\_assuming\\_random\\_guessing$.  \n",
    "\n",
    "The curves below correspond to training data between 2017-02-01 and 2017-07-31 (the last available date in the listings). Locations have been discretized using a 10x10 grid. Validation data has a date range ov 2017-01-01 and 2017-01-31 and is unbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric f1: harmonic mean between recall and precision. \n",
    "![f1](img/f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric f_f1: harmonic mean between recall and precision. Here, we have plotted how much this measure is above the pure random approach using just the probability of positive samples in the valuation dataset.\n",
    "![f_f1](img/f_f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric precision \n",
    "![prec](img/prec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric f_precision \n",
    "![f_prec](img/f_prec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric recall \n",
    "![reca](img/reca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric f_recall \n",
    "![f_reca](img/f_reca.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
